import numpy as np
import sklearn
from sklearn.tree import DecisionTreeClassifier

class RandomForest(object):
    def __init__(self, n_estimators=2000, max_depth=2, max_features=1):
        # No need to modify this function!
        """
        Initializer function for Random Forest.

        n_estimators:    Number of decision trees that will be used to create the forest.
        max_depth:       Max depth allowed for each decision tree.
        max_features:    Percentage of features that are used to fit each decision tree.
        bootstrap_sample_indices:   Sample indicies chosen for this tree.
        bootstrap_feature_indices:  Feature indicies chosen for this tree.
        decision_trees:  The decision tree data structure.
                         Generated by sklearn DecisionTreeClassifier.
        """

        self.n_estimators = n_estimators
        self.max_depth = max_depth
        self.max_features = max_features
        self.bootstrap_sample_indices = []
        self.bootstrap_feature_indices = []
        self.out_of_bag = []
        self.n_features = 8
        self.n_samples = 100
        self.decision_trees = [
            sklearn.tree.DecisionTreeClassifier(max_depth=max_depth,
                                                criterion='entropy')
            for i in range(n_estimators)]

    def _bootstrapping(self, num_samples, num_features, random_seed = None):
        """
        Args:
            num_samples:  Number of samples in the input dataset.
                          Will sample (num_samples) with replacement.
            num_features: Number of features in the input dataset.
                          Will sample (num_features * self.max_features) without replacement.

        Return:
            row_idx: List of row indices which correspond to the row locations
                     of the selected samples in the original dataset.
            col_idx: List of column indices which correspond to the column locations
                     of the selected features in the original dataset.

        Example:
            Input: random_samples=5, num_features=10, where self.max_features=0.8.
            Note, this is a random example of what a random output could look like.

            row_idx = [2, 0, 0, 3, 4]
            col_idx = [9, 3, 4, 7, 8, 2, 0, 6]

        Reference: https://en.wikipedia.org/wiki/Bootstrapping_(statistics)

        Hint: Consider using np.random.choice().
        """

        np.random.seed(seed=random_seed)
        row_idx = np.random.choice(num_samples, num_samples, replace=True)
        col_idx = np.random.choice(num_features, int(self.max_features * num_features), replace=False)
        return row_idx, col_idx

    def bootstrapping(self, num_samples, num_features):
        # No need to modify this function!
        for i in range(self.n_estimators):
            total = set(list(range(num_samples)))
            row_idx, col_idx = self._bootstrapping(num_samples, num_features)
            total = total - set(row_idx)
            self.bootstrap_sample_indices.append(row_idx)
            self.bootstrap_feature_indices.append(col_idx)
            self.out_of_bag.append(total)

    def fit(self, X, y):
        """
        Train decision trees using the bootstrapped datasets.
        Note, will use self.bootstrap_sample_indices,
                       self.bootstrap_feature_indices,
                       self.n_estimators,
                       self.decision_trees.

        Args:
            X: NxD numpy array, where N is number
               of instances and D is the dimensionality of each
               instance
            y: Nx1 numpy array, the predicted labels

        Return:
            No return value. Complete decision tree is stored in self.decision_trees.

        """
        self.n_samples, self.n_features = X.shape  # Retain this line.

        if(type(y) is list):
            y = np.array(y, dtype="object")
        if(type(X) is list):
            X = np.array(X, dtype="object")
        self.bootstrapping(X.shape[0], X.shape[1])
        for index in range(self.n_estimators):
            Xtemp = X[self.bootstrap_sample_indices[index], :]
            Xtemp = Xtemp[:, self.bootstrap_feature_indices[index]]
            Ytemp = y[self.bootstrap_sample_indices[index]]
            self.decision_trees[index].fit(Xtemp, Ytemp)


    def OOB_score(self, X, y):
        # No need to modify this function!
        # Computes the accuracy of the random forest model predicting y given x.
        accuracy = []
        for i in range(len(X)):
            predictions = []
            for t in range(self.n_estimators):
                if i in self.out_of_bag[t]:
                    predictions.append(self.decision_trees[t].predict(np.reshape(X[i][self.bootstrap_feature_indices[t]], (1,-1)))[0])
            if len(predictions) > 0:
                accuracy.append(np.sum(predictions == y[i]) / float(len(predictions)))
        return np.mean(accuracy)


    def average_feature_importance(self):
        """
        Gets the average feature importance for this random forest model.

        Returns:
            avg_feature_importance: Average feature importance. Found by aggregating
                                    the feature importance of every decision tree in
                                    the model, then dividing by how many times each
                                    feature was selected in the model.

        Hint: Use self.decision_trees[i].feature_importances_
                  self.feature_indices[i]
                  self.n_features
        """
        imp = 0
        hwm = 0
        for i in range(self.n_estimators):
            imp = imp + self.decision_trees[i].feature_importances_
            hwm= hwm + self.bootstrap_feature_indices[i]
        return hwm/imp
        



